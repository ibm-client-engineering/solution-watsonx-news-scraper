{"version":3,"file":"component---src-pages-technical-solution-create-create-mdx-629566f49bc7ce756ef8.js","mappings":"yPAMaA,EAAe,CAAC,EACvBC,EAAc,CAClBD,gBAEIE,EAAYC,EAAAA,EACH,SAASC,EAAUC,GAG/B,IAHgC,WACjCC,GAEDD,EADIE,GAAKC,EAAAA,EAAAA,GAAAH,EAAAI,GAER,OAAOC,EAAAA,EAAAA,IAACR,EAASS,OAAAC,OAAA,GAAKX,EAAiBM,EAAK,CAAED,WAAYA,EAAYO,QAAQ,eAG5EH,EAAAA,EAAAA,IAAA,UAAM,wBACNA,EAAAA,EAAAA,IAAA,SAAK,4LAA0LA,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,mBAA4B,sSACpPJ,EAAAA,EAAAA,IAAA,YAAKA,EAAAA,EAAAA,IAAA,QAAMI,WAAW,MAClB,UAAa,iBACV,4qCAEPJ,EAAAA,EAAAA,IAAA,SAAK,oOACLA,EAAAA,EAAAA,IAAA,YAAKA,EAAAA,EAAAA,IAAA,QAAMI,WAAW,MAClB,UAAa,mBACV,olBAYPJ,EAAAA,EAAAA,IAAA,SAAK,4XACLA,EAAAA,EAAAA,IAAA,YAAKA,EAAAA,EAAAA,IAAA,QAAMI,WAAW,MAClB,UAAa,mBACV,uvBAsBPJ,EAAAA,EAAAA,IAAA,SAAK,kIACLA,EAAAA,EAAAA,IAAA,UAAM,kBAAgBA,EAAAA,EAAAA,IAAA,QAAMK,MAAO,CAC/B,MAAS,SACN,OACPL,EAAAA,EAAAA,IAAA,SAAK,oCAAkCA,EAAAA,EAAAA,IAAA,QAAMK,MAAO,CAChD,MAAS,SACN,KAAY,iGAA+FL,EAAAA,EAAAA,IAAA,QAAMK,MAAO,CAC3H,MAAS,SACN,KAAY,2GAAyGL,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,UAAmB,IAAK,IAAK,KAAGJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,WAAoB,iCAClOJ,EAAAA,EAAAA,IAAA,SAAK,0BAAwBA,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,UAAmB,0BAAwBJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,6BAAsC,8CAA4CJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,gBAAyB,gBAAcJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,qBAA8B,aAAWJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,yBAAkC,MACzYJ,EAAAA,EAAAA,IAAA,SAAK,uIAAqIA,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,UAAmB,oCAAkCJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,UAAmB,IAAK,IAAK,KAAGJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,gBAAyB,IAAK,IAAK,KAAGJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,YAAqB,gBAAcJ,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,YAAqB,6NAC1bJ,EAAAA,EAAAA,IAAA,SAAK,kDAAgDA,EAAAA,EAAAA,IAAA,UAAQI,WAAW,KAAM,kBAA2B,uBACzGJ,EAAAA,EAAAA,IAAA,YAAKA,EAAAA,EAAAA,IAAA,QAAMI,WAAW,OAAgB,gLAOtCJ,EAAAA,EAAAA,IAAA,SAAK,8DAA4DA,EAAAA,EAAAA,IAAA,QAAMK,MAAO,CAC1E,MAAS,SACN,KAAY,MACnBL,EAAAA,EAAAA,IAAA,UAAM,sBACNA,EAAAA,EAAAA,IAAA,SAAK,sJAAoJA,EAAAA,EAAAA,IAAA,KAAGI,WAAW,IACnK,KAAQ,6BACL,2BAA+B,gJAG1C,CAEAV,EAAWY,gBAAiB,C","sources":["webpack://website/./src/pages/technical-solution/create/create.mdx"],"sourcesContent":["import * as React from 'react'\n  /* @jsx mdx */\nimport { mdx } from '@mdx-js/react';\n/* @jsxRuntime classic */\n/* @jsx mdx */\nimport DefaultLayout from \"/home/runner/work/solution-watsonx-news-scraper/solution-watsonx-news-scraper/node_modules/gatsby-theme-carbon/src/templates/Default.js\";\nexport const _frontmatter = {};\nconst layoutProps = {\n  _frontmatter\n};\nconst MDXLayout = DefaultLayout;\nexport default function MDXContent({\n  components,\n  ...props\n}) {\n  return <MDXLayout {...layoutProps} {...props} components={components} mdxType=\"MDXLayout\">\n\n\n    <h2>{`Part 1: Webscraping`}</h2>\n    <p>{`The first part of this project ecompasses retrieving articles from online news sources. The first step is to visit the website you would like to scrape from and observe the html using `}<strong parentName=\"p\">{`inspect element`}</strong>{`. For this example, we will be scraping from the CNN economy news section. From this page, we want to retrieve all of the links on the webpage that lead to articles, so we need to find a common tag that every headline link shares. The element for a headline might look something like this:`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-html\"\n      }}>{`<a href=\"/2024/02/01/economy/january-jobs-report-preview/index.html\" class=\"container__link container__link--type-article container_lead-plus-headlines-with-images__link\" data-link-type=\"article\" data-zjs=\"click\" data-zjs-cms_id=\"cms.cnn.com/_pages/cl8lvnljb0000ldp3b79d6s4l@published\" data-zjs-canonical_url=\"https://www.cnn.com/business/economy\" data-zjs-zone_id=\"cms.cnn.com/_components/zone/instances/cl8lvnlna0013ldp3vxczixnk@published\" data-zjs-zone_name=\"undefined\" data-zjs-zone_type=\"zone_layout--5-4-3\" data-zjs-zone_position_number=\"1\" data-zjs-zone_total_number=\"3\" data-zjs-container_id=\"cms.cnn.com/_components/container/instances/cl8lvnlna0014ldp32buq116y@published\" data-zjs-container_name=\"undefined\" data-zjs-container_type=\"container_lead-plus-headlines-with-images\" data-zjs-container_position_number=\"1\" data-zjs-container_total_number=\"3\" data-zjs-card_id=\"cms.cnn.com/_components/card/instances/cl8lvnlna0014ldp32buq116y_fill_1@published\" data-zjs-card_name=\"Fed&nbsp;Chair&nbsp;Powell&nbsp;says the job market is still strong. Here’s what to know about the numbers\" data-zjs-card_type=\"card\" data-zjs-card_position_number=\"1\" data-zjs-card_total_number=\"8\">...</a>\n`}</code></pre>\n    <p>{`Within this HTML element we can see that the partial link to the article is embedded under the href attribute. With this information, we can now write python code using BeautifulSoup to extract the headlines from this page.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`def scrape_cnn() :\n    cnn_URL = \"https://www.cnn.com/business/economy\"\n    cnn_page = requests.get(cnn_URL)\n\n    cnn_soup = BeautifulSoup(cnn_page.content, \"html.parser\")\n    cnn_articles = cnn_soup.find_all('div', {'class': 'container_lead-plus-headlines-with-images__item'})\n\n    cnn_links = []\n    for article in cnn_articles :\n        if not any(avoided in article['data-open-link'] for avoided in ['live', 'videos']) : #skip live articles and videos for now they have a different format\n            cnn_links.append(\"https://www.cnn.com\" + article['data-open-link'])\n`}</code></pre>\n    <p>{`Now we can move on to extracting information from the articles. For each article we are interested in three things: the title, the date it was published, and the article’s content. We repeat a very similar process to find the html elements associated with each of these things, and we consolidate them into a list. We repeat this step for every single article link collected.`}</p>\n    <pre><code parentName=\"pre\" {...{\n        \"className\": \"language-python\"\n      }}>{`cnn_articles= []\n\n    for link in cnn_links:\n\n        page = requests.get(link)\n\n        soup = BeautifulSoup(page.content, \"html.parser\")\n        cnn_title = soup.find('h1', {'class': 'headline__text inline-placeholder'})\n        cnn_content = soup.find('div', {'class': 'article__content-container'})\n        timestamp = soup.find('div', {'class': 'timestamp'})\n        timestr = timestamp.text\n        digits = re.search(r\"\\\\d\", timestr)\n        dt = parser.parse(timestr[digits.start(0):])\n\n        try :\n            cnn_articles.append(['CNN', remove_formatting(cnn_title.text),remove_formatting(cnn_content.text), dt])\n        \n        except:\n            print(\"This article has no text!\")\n    \n    return cnn_articles\n`}</code></pre>\n    <p>{`The try catch block here is to handle cases where the article may have a slightly different format or include no text at all.`}</p>\n    <h2>{`Part 2: Watson`}<span style={{\n        \"color\": \"blue\"\n      }}>{`x`}</span></h2>\n    <p>{`Now we need to connect to watson`}<span style={{\n        \"color\": \"blue\"\n      }}>{`x`}</span>{` in order to analyze the news articles that we have collected. Once you have created a watson`}<span style={{\n        \"color\": \"blue\"\n      }}>{`x`}</span>{` account, create a new empty project for the webscraper. In the new project, find the Project ID under `}<strong parentName=\"p\">{`Manage`}</strong>{` `}{`→`}{` `}<strong parentName=\"p\">{`General`}</strong>{`, and save it for later use.`}</p>\n    <p>{`While still under the `}<strong parentName=\"p\">{`Manage`}</strong>{` header, click on the `}<strong parentName=\"p\">{`Services and integrations`}</strong>{` section on the left hand side. Under the `}<strong parentName=\"p\">{`IBM Services`}</strong>{` tab, click `}<strong parentName=\"p\">{`Associate Service`}</strong>{` and add `}<strong parentName=\"p\">{`WatsonMachineLearning`}</strong>{`.`}</p>\n    <p>{`Once the project has been created, we need to create an API key for our demo to use. Return to the IBM Cloud homepage and find the `}<strong parentName=\"p\">{`Manage`}</strong>{` header on the top. Navigate to `}<strong parentName=\"p\">{`Manage`}</strong>{` `}{`→`}{` `}<strong parentName=\"p\">{`Access (IAM)`}</strong>{` `}{`→`}{` `}<strong parentName=\"p\">{`API Keys`}</strong>{` and select `}<strong parentName=\"p\">{`Create +`}</strong>{`. Give the new API key a clear name such as “Watsonx Webscraper Demo”, and a description if you would like. Once the key has been created, save it somewhere secure, as you will not be able to retrieve the key again. `}</p>\n    <p>{`Now create a file in the assets folder called `}<strong parentName=\"p\">{`API_creds.json`}</strong>{` with this format:`}</p>\n    <pre><code parentName=\"pre\" {...{}}>{`{\n    \"BAM_Key\": PUT YOUR BAM KEY HERE,\n    \"BAM_URL\": PUT YOUR BAM URL HERE,\n    \"WX_Key\" : PUT YOUR WX KEY HERE,\n    \"WX_Project\" : PUT YOUR WX PROJECT ID HERE\n}\n`}</code></pre>\n    <p>{`The BAM key and url can be ignored if you are using watson`}<span style={{\n        \"color\": \"blue\"\n      }}>{`x`}</span>{`.`}</p>\n    <h2>{`Part 3: Streamlit`}</h2>\n    <p>{`The GUI for the app was created using streamlit. Streamlit is an easy to use python package for creating browser-based applications. Refer to the `}<a parentName=\"p\" {...{\n        \"href\": \"https://docs.streamlit.io\"\n      }}>{`streamlit documentation`}</a>{` to get started and learn more about how to use streamlit, both Articles.py and Bank_Exec.py in the /assets/ folder use streamlit frontends.`}</p>\n\n    </MDXLayout>;\n}\n;\nMDXContent.isMDXComponent = true;\n      "],"names":["_frontmatter","layoutProps","MDXLayout","DefaultLayout","MDXContent","_ref","components","props","_objectWithoutProperties","_excluded","mdx","Object","assign","mdxType","parentName","style","isMDXComponent"],"sourceRoot":""}